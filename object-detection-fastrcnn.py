# -*- coding: utf-8 -*-
"""DS 541 - FP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ttBblwKxkHyrrXnZm6C_pxz7l6vzeqxa

## Libraries
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import json
import cv2
import shutil
import random
import subprocess
import sys
import yaml
from pathlib import Path
from PIL import Image
import time

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, auc, precision_recall_curve, mean_absolute_error

import torch
import torch.nn as nn
import torchvision
import torchvision.models as models
import torchvision.transforms as T
import torch.optim as optim
from torch.optim import Adam, SGD
from torch.optim.lr_scheduler import StepLR
from torch.utils.data import Dataset, DataLoader, TensorDataset
from torchvision import transforms
from torchvision.ops import box_iou
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.backbone_utils import resnet_fpn_backbone
from torchvision.models.detection import maskrcnn_resnet50_fpn
from torchmetrics.detection.mean_ap import MeanAveragePrecision

from detectron2.engine import DefaultTrainer, DefaultPredictor
from detectron2.config import get_cfg
from detectron2 import model_zoo
from detectron2.data import DatasetCatalog, MetadataCatalog
from detectron2.utils.visualizer import Visualizer
from detectron2.data.datasets import register_coco_instances

from pycocotools.coco import COCO

df = pd.read_csv('Malaria.csv')
df

# Image folder and annotations folder
#images_folder = 'images'
#annotations_folder = 'annotations'

"""## Object Detection 1"""

'''

class CustomDataset(Dataset):
    def __init__(self, image_dir, annotation_file, transforms=None):
        """
        Args:
            image_dir (str): Path to the directory with images.
            annotation_file (str): Path to the updated_annotations.json file.
            transforms (callable, optional): Optional transform to be applied on a sample.
        """
        self.image_dir = image_dir
        self.transforms = transforms

        # Load the annotations
        with open(annotation_file, 'r') as f:
            self.annotations = json.load(f)

    def __len__(self):
        return len(self.annotations)

    def __getitem__(self, idx):
        # Get image info and annotation
        img_info = self.annotations[idx]
        img_id = img_info["image_id"]
        img_path = os.path.join(self.image_dir, img_id)
        image = Image.open(img_path).convert("RGB")

        # Get bounding boxes and labels
        boxes = []
        labels = []
        for ann in img_info["annotations"]:
            bbox = ann["bbox"]
            category_id = ann["category_id"]

            # Convert to xyxy format
            xmin, ymin, width, height = bbox
            xmax = xmin + width
            ymax = ymin + height
            boxes.append([xmin, ymin, xmax, ymax])
            labels.append(category_id)

        # Convert to tensors
        boxes = torch.tensor(boxes, dtype=torch.float32)
        labels = torch.tensor(labels, dtype=torch.int64)

        # Create target dict
        target = {
            "boxes": boxes,
            "labels": labels,
            "image_id": torch.tensor([idx]),
            "area": torch.tensor([bbox[2] * bbox[3] for bbox in boxes], dtype=torch.float32),
            "iscrowd": torch.zeros(len(boxes), dtype=torch.int64)
        }

        if self.transforms:
            image = self.transforms(image)

        return image, target

# Define transforms for data augmentation
transform = transforms.Compose([
    transforms.ToTensor(),
])

# For the training dataset:
train_image_dir = "train/images"
train_annotation_file = "train/updated_annotations.json"
train_dataset = CustomDataset(train_image_dir, train_annotation_file, transforms=transform)

# For validation and test datasets:
val_image_dir = "val/images"
val_annotation_file = "val/updated_annotations.json"
val_dataset = CustomDataset(val_image_dir, val_annotation_file, transforms=transform)

test_image_dir = "test/images"
test_annotation_file = "test/updated_annotations.json"
test_dataset = CustomDataset(test_image_dir, test_annotation_file, transforms=transform)

# Define the model with a pre-trained ResNet50 backbone with FPN
model = fasterrcnn_resnet50_fpn(pretrained=True)

# Replace the classifier to fit your number of classes (7 classes including background)
num_classes = 7  # 6 classes + 1 for background
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)

# Move the model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Define data loaders
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))
test_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))

# Set up optimizer and learning rate scheduler
optimizer = Adam(model.parameters(), lr=1e-4)
lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)

# Initialize lists to store loss values
train_losses = []
val_losses = []

# Training loop
num_epochs = 10

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    for images, targets in train_loader:
        images = [image.to(device) for image in images]

        # Ensure all target tensors are on the same device
        for i, target in enumerate(targets):
            target["boxes"] = target["boxes"].to(device)
            target["labels"] = target["labels"].to(device)
            target["image_id"] = target["image_id"].to(device)
            target["area"] = target["area"].to(device)
            target["iscrowd"] = target["iscrowd"].to(device)

            # Handle case where there are no boxes (empty annotations)
            if target["boxes"].size(0) == 0:  # If no boxes, set shape to [0, 4]
                target["boxes"] = torch.zeros((0, 4), dtype=torch.float32).to(device)
                target["labels"] = torch.zeros((0,), dtype=torch.int64).to(device)
                target["area"] = torch.zeros((0,), dtype=torch.float32).to(device)
                target["iscrowd"] = torch.zeros((0,), dtype=torch.int64).to(device)

        optimizer.zero_grad()
        loss_dict = model(images, targets)

        # If the output is a dictionary
        if isinstance(loss_dict, dict):
            loss = sum(loss for loss in loss_dict.values())
        # If the output is a list
        elif isinstance(loss_dict, list):
            loss = sum(loss_dict)  # Sum the list of losses
        else:
            raise ValueError("Unexpected output format from model")

        loss.backward()
        optimizer.step()

        train_loss += loss.item()

    # Average training loss for the epoch
    train_loss /= len(train_loader)
    train_losses.append(train_loss)

    # Validation loop (similar to the training loop)
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for images, targets in val_loader:
            images = [image.to(device) for image in images]

            # Ensure all target tensors are on the same device
            for i, target in enumerate(targets):
                target["boxes"] = target["boxes"].to(device)
                target["labels"] = target["labels"].to(device)
                target["image_id"] = target["image_id"].to(device)
                target["area"] = target["area"].to(device)
                target["iscrowd"] = target["iscrowd"].to(device)

                # Handle case where there are no boxes (empty annotations)
                if target["boxes"].size(0) == 0:  # If no boxes, set shape to [0, 4]
                    target["boxes"] = torch.zeros((0, 4), dtype=torch.float32).to(device)
                    target["labels"] = torch.zeros((0,), dtype=torch.int64).to(device)
                    target["area"] = torch.zeros((0,), dtype=torch.float32).to(device)
                    target["iscrowd"] = torch.zeros((0,), dtype=torch.int64).to(device)

            # If the output is a dictionary (as expected in Faster R-CNN)
            if isinstance(loss_dict, dict):
              # Sum the loss values (e.g., loss_classifier, loss_box_reg, etc.)
                loss = sum(loss.item() for loss in loss_dict.values())
            elif isinstance(loss_dict, list):
            # If it's a list, sum the elements (usually not the case in Faster R-CNN)
                loss = sum(loss.item() for loss in loss_dict)
            else:
                 raise ValueError("Unexpected output format from model")

            val_loss += loss

    # Average validation loss for the epoch
    val_loss /= len(val_loader)
    val_losses.append(val_loss)

    print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

# Plot the training and validation losses
plt.figure(figsize=(10, 6))
plt.plot(range(1, num_epochs + 1), train_losses, label="Train Loss")
plt.plot(range(1, num_epochs + 1), val_losses, label="Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training and Validation Losses")
plt.legend()
plt.show()

# Initialize Mean Average Precision (mAP) metric
map_metric = MeanAveragePrecision()

# Evaluate on test data
model.eval()
all_targets = []
all_preds = []

with torch.no_grad():
    for images, targets in test_loader:
        images = [image.to(device) for image in images]  # Ensure images are on the same device
        preds = model(images)  # Model predictions

        # Handle empty annotations during evaluation
        for i, target in enumerate(targets):
            if len(target["boxes"]) == 0:
                targets[i]["boxes"] = torch.zeros((0, 4), dtype=torch.float32).to(device)
                targets[i]["labels"] = torch.zeros((0,), dtype=torch.int64).to(device)

        # Move model predictions and targets to the correct device (GPU or CPU)
        formatted_preds = [
            {
                "boxes": pred["boxes"].to(device).cpu(),  # Move to device first, then cpu for mAP metric
                "scores": pred["scores"].to(device).cpu(),
                "labels": pred["labels"].to(device).cpu(),
            }
            for pred in preds
        ]

        formatted_targets = [
            {
                "boxes": target["boxes"].to(device).cpu(),  # Move to device first, then cpu
                "labels": target["labels"].to(device).cpu(),
            }
            for target in targets
        ]

        # Add predictions and targets to the mAP metric
        map_metric.update(formatted_preds, formatted_targets)

        # Store for Precision-Recall curve
        all_targets.extend(formatted_targets)
        all_preds.extend(formatted_preds)

# Compute and print mAP
map_results = map_metric.compute()
print(f"mAP: {map_results['map']:.4f}")
print(f"mAP (50% IoU): {map_results['map_50']:.4f}")
print(f"mAP (75% IoU): {map_results['map_75']:.4f}")

# Initialize lists to store the precision-recall curves for each class
class_ids = [0, 2, 3, 4, 5, 6]  # Replace with your actual class ids
for class_id in class_ids:
    # Get ground truth and predicted scores for the current class
    gt_labels = [1 if gt["labels"] == class_id else 0 for gt in all_targets]
    pred_scores = [pred["scores"] for pred in all_preds]

    # Compute Precision-Recall curve for the current class
    precision, recall, thresholds = precision_recall_curve(gt_labels, pred_scores)

    # Calculate the AUC for the current class
    pr_auc = auc(recall, precision)

    # Plot Precision-Recall curve for the current class
    plt.plot(recall, precision, label=f"Class {class_id} (AUC = {pr_auc:.4f})")

# Add labels and show the plot
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve for Multi-Class")
plt.legend()
plt.show()

'''

# Data for the cell types
cell_types = ['Red Blood Cell', 'Leukocyte', 'Trophozoite', 'Schizont', 'Ring', 'Gametocyte']
counts = [83034, 103, 1584, 190, 522, 156]

# Define colors for each bar
colors = ['#ff4c4c', '#ffcc00', '#3399ff', '#4caf50', '#ff8c1a', '#8a2be2']

# Create the bar plot
plt.figure(figsize=(10, 6))
bars = plt.bar(cell_types, counts, color=colors)

# Add labels and title
plt.xlabel('Cell Types', fontsize=12)
plt.ylabel('Counts', fontsize=12)
plt.title('Cell Counts by Type', fontsize=14)

# Add the count labels on top of each bar
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 100, int(yval), ha='center', va='bottom', fontsize=10)

# Show the plot
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Data for the cell types
cell_types = ['Red Blood Cell', 'Leukocyte', 'Trophozoite', 'Schizont', 'Ring', 'Gametocyte']
counts = [83034, 103, 1584, 190, 522, 156]

# Calculate the total count
total_count = sum(counts)

# Calculate the percentage for each cell type
percentages = [(count / total_count) * 100 for count in counts]

# Define colors for each bar
colors = ['#ff4c4c', '#ffcc00', '#3399ff', '#4caf50', '#ff8c1a', '#8a2be2']

# Create the bar plot
plt.figure(figsize=(10, 6))
bars = plt.bar(cell_types, percentages, color=colors)

# Add labels and title
plt.xlabel('Cell Types', fontsize=12)
plt.ylabel('Percentage (%)', fontsize=12)
plt.title('Percentage of Each Cell Type', fontsize=14)

# Add the percentage labels on top of each bar
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.5, f'{yval:.2f}%', ha='center', va='bottom', fontsize=10)

# Show the plot
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Data: Training and Validation Loss for each epoch
epochs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
train_loss = [0.2265, 0.1010, 0.0852, 0.0745, 0.0657, 0.0588, 0.0536, 0.0565, 0.0460, 0.0408]
val_loss = [0.1717, 0.0000, 0.0852, 0.0321, 0.0461, 0.0371, 0.0450, 0.1142, 0.0292, 0.0317]

# Plotting the training and validation loss
plt.figure(figsize=(10, 6))
plt.plot(epochs, train_loss, label='Training Loss', color='b', marker='o', linestyle='-', linewidth=2, markersize=6)
plt.plot(epochs, val_loss, label='Validation Loss', color='r', marker='o', linestyle='-', linewidth=2, markersize=6)

# Adding labels and title
plt.xlabel('Epochs', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.title('Training and Validation Loss Over Epochs', fontsize=14)
plt.legend()

# Display the plot
plt.grid(True)
plt.tight_layout()
plt.show()

# Mapping from categories to class_ids (expand subcategories)
category_map = {
    "red blood cell": 0,         # Uninfected
    "leukocyte": 2,              # Uninfected (Leukocyte)
    "trophozoite_infected": 3,   # Infected (Trophozoite)
    "schizont_infected": 4,      # Infected (Schizont)
    "ring_infected": 5,          # Infected (Ring)
    "gametocyte_infected": 6     # Infected (Gametocyte)
}